{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V100","mount_file_id":"1OvXVIqCJJC6jkwSrLrdBpXPyGHInQkKp","authorship_tag":"ABX9TyNcCofhuk1LW61cLt/x9POf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 모델"],"metadata":{"id":"RmYqqaRd8JSv"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"xkRfqGPPZhh0","executionInfo":{"status":"error","timestamp":1703612015666,"user_tz":-540,"elapsed":1985891,"user":{"displayName":"곽한빈","userId":"12753390330832787926"}},"outputId":"6574b389-d2fe-4598-d5c9-ab83c390f6b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5755 images belonging to 5 classes.\n","Found 1437 images belonging to 5 classes.\n","Epoch 1/20\n","180/180 [==============================] - ETA: 0s - loss: 0.3326 - accuracy: 0.8862"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: val_accuracy improved from -inf to 0.16075, saving model to /content/drive/MyDrive/Python_project/2023.12.26 프로젝트/KWAK/Cross_validation_checkpoint\n","180/180 [==============================] - 169s 738ms/step - loss: 0.3326 - accuracy: 0.8862 - val_loss: 4.0288 - val_accuracy: 0.1608 - lr: 1.0000e-04\n","Epoch 2/20\n","180/180 [==============================] - ETA: 0s - loss: 0.1296 - accuracy: 0.9526"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 2: val_accuracy improved from 0.16075 to 0.19624, saving model to /content/drive/MyDrive/Python_project/2023.12.26 프로젝트/KWAK/Cross_validation_checkpoint\n","180/180 [==============================] - 130s 725ms/step - loss: 0.1296 - accuracy: 0.9526 - val_loss: 3.5977 - val_accuracy: 0.1962 - lr: 1.0000e-04\n","Epoch 3/20\n","180/180 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9731"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 3: val_accuracy improved from 0.19624 to 0.31454, saving model to /content/drive/MyDrive/Python_project/2023.12.26 프로젝트/KWAK/Cross_validation_checkpoint\n","180/180 [==============================] - 131s 728ms/step - loss: 0.0764 - accuracy: 0.9731 - val_loss: 3.6090 - val_accuracy: 0.3145 - lr: 1.0000e-04\n","Epoch 4/20\n","180/180 [==============================] - ETA: 0s - loss: 0.0802 - accuracy: 0.9748"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 4: val_accuracy improved from 0.31454 to 0.44816, saving model to /content/drive/MyDrive/Python_project/2023.12.26 프로젝트/KWAK/Cross_validation_checkpoint\n","180/180 [==============================] - 130s 724ms/step - loss: 0.0802 - accuracy: 0.9748 - val_loss: 3.0639 - val_accuracy: 0.4482 - lr: 1.0000e-04\n","Epoch 5/20\n","180/180 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9772"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 5: val_accuracy improved from 0.44816 to 0.70146, saving model to /content/drive/MyDrive/Python_project/2023.12.26 프로젝트/KWAK/Cross_validation_checkpoint\n","180/180 [==============================] - 129s 718ms/step - loss: 0.0674 - accuracy: 0.9772 - val_loss: 1.3190 - val_accuracy: 0.7015 - lr: 1.0000e-04\n","Epoch 6/20\n","180/180 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.9807"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 6: val_accuracy improved from 0.70146 to 0.83925, saving model to /content/drive/MyDrive/Python_project/2023.12.26 프로젝트/KWAK/Cross_validation_checkpoint\n","180/180 [==============================] - 130s 722ms/step - loss: 0.0535 - accuracy: 0.9807 - val_loss: 0.5780 - val_accuracy: 0.8392 - lr: 1.0000e-04\n","Epoch 7/20\n","180/180 [==============================] - ETA: 0s - loss: 0.0599 - accuracy: 0.9823"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 7: val_accuracy improved from 0.83925 to 0.86987, saving model to /content/drive/MyDrive/Python_project/2023.12.26 프로젝트/KWAK/Cross_validation_checkpoint\n","180/180 [==============================] - 131s 729ms/step - loss: 0.0599 - accuracy: 0.9823 - val_loss: 0.4793 - val_accuracy: 0.8699 - lr: 1.0000e-04\n","Epoch 8/20\n","180/180 [==============================] - ETA: 0s - loss: 0.0495 - accuracy: 0.9847"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 8: val_accuracy did not improve from 0.86987\n","180/180 [==============================] - 106s 591ms/step - loss: 0.0495 - accuracy: 0.9847 - val_loss: 0.7232 - val_accuracy: 0.8267 - lr: 1.0000e-04\n","Epoch 9/20\n","180/180 [==============================] - ETA: 0s - loss: 0.0607 - accuracy: 0.9786\n","Epoch 9: val_accuracy did not improve from 0.86987\n","\n","Epoch 9: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n","180/180 [==============================] - 107s 593ms/step - loss: 0.0607 - accuracy: 0.9786 - val_loss: 0.7205 - val_accuracy: 0.8065 - lr: 1.0000e-04\n","Epoch 10/20\n","180/180 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9937\n","Epoch 10: val_accuracy improved from 0.86987 to 0.88379, saving model to /content/drive/MyDrive/Python_project/2023.12.26 프로젝트/KWAK/Cross_validation_checkpoint\n","180/180 [==============================] - 129s 719ms/step - loss: 0.0216 - accuracy: 0.9937 - val_loss: 0.4953 - val_accuracy: 0.8838 - lr: 2.0000e-05\n","Epoch 11/20\n","180/180 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.9979"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 11: val_accuracy improved from 0.88379 to 0.90257, saving model to /content/drive/MyDrive/Python_project/2023.12.26 프로젝트/KWAK/Cross_validation_checkpoint\n","180/180 [==============================] - 131s 727ms/step - loss: 0.0088 - accuracy: 0.9979 - val_loss: 0.4461 - val_accuracy: 0.9026 - lr: 2.0000e-05\n","Epoch 12/20\n","180/180 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 0.9983"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 12: val_accuracy did not improve from 0.90257\n","180/180 [==============================] - 107s 594ms/step - loss: 0.0073 - accuracy: 0.9983 - val_loss: 0.4649 - val_accuracy: 0.8984 - lr: 2.0000e-05\n","Epoch 13/20\n","180/180 [==============================] - ETA: 0s - loss: 0.0054 - accuracy: 0.9991\n","Epoch 13: val_accuracy did not improve from 0.90257\n","\n","Epoch 13: ReduceLROnPlateau reducing learning rate to 1e-05.\n","180/180 [==============================] - 106s 588ms/step - loss: 0.0054 - accuracy: 0.9991 - val_loss: 0.5164 - val_accuracy: 0.8914 - lr: 2.0000e-05\n","Epoch 14/20\n","180/180 [==============================] - ETA: 0s - loss: 0.0046 - accuracy: 0.9988\n","Epoch 14: val_accuracy did not improve from 0.90257\n","180/180 [==============================] - 107s 595ms/step - loss: 0.0046 - accuracy: 0.9988 - val_loss: 0.4676 - val_accuracy: 0.8970 - lr: 1.0000e-05\n","Epoch 15/20\n","180/180 [==============================] - ETA: 0s - loss: 0.0045 - accuracy: 0.9988\n","Epoch 15: val_accuracy did not improve from 0.90257\n","180/180 [==============================] - 107s 593ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.5064 - val_accuracy: 0.8956 - lr: 1.0000e-05\n","Epoch 16/20\n","180/180 [==============================] - ETA: 0s - loss: 0.0028 - accuracy: 0.9993\n","Epoch 16: val_accuracy did not improve from 0.90257\n","180/180 [==============================] - 107s 593ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.5098 - val_accuracy: 0.8900 - lr: 1.0000e-05\n","Epoch 16: early stopping\n","45/45 [==============================] - 22s 477ms/step - loss: 0.5264 - accuracy: 0.8970\n","Validation Loss: 0.5263601541519165\n","Validation Accuracy: 0.8970076441764832\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"error","ename":"IsADirectoryError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-88af1922756c>\u001b[0m in \u001b[0;36m<cell line: 296>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;31m# model.save: 전체 모델(아키텍처, 가중치, 훈련 구성, 옵티마이저 상태 등)을\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;31m# 단일 파일에 저장하여 지정 경로에 모델 파일을 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_save_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;31m# ====================================================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    565\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 567\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n","\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Unable to create file (unable to open file: name = '/content/drive/MyDrive/Python_project/2023.12.26 프로젝트/KWAK/model_Res_Net/resnet_model.h5', errno = 21, error message = 'Is a directory', flags = 13, o_flags = 242)"]}],"source":["# < 라이브러리 임포트 >\n","import os\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.applications import ResNet50, EfficientNetB0\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n","\n","# ====================================================================================================\n","\n","# < 데이터셋 경로 >\n","dataset_path = '/content/drive/MyDrive/Python_project/2023.12.26 프로젝트/data/current_dataset'\n","\n","# ====================================================================================================\n","\n","# < 이미지 크기 및 배치 크기 설정 >\n","\n","# ResNet의 기본 이미지 크기\n","img_width, img_height = 224, 224\n","batch_size = 32\n","\n","# ====================================================================================================\n","\n","# < 이미지 데이터 생성기 >\n","train_datagen = ImageDataGenerator(\n","\n","    # 모든 이미지 픽셀 값을 [0, 255]에서 [0, 1] 범위로 스케일링\n","    rescale=1./255,\n","\n","    # 0도에서 10도 사이로 임의의 각도로 이미지를 회전\n","    rotation_range=10,\n","\n","    # 이미지를 수평으로 최대 이미지 너비의 10%까지 임의로 이동\n","    width_shift_range=0.1,\n","\n","    # 이미지를 수직으로 최대 이미지 높이의 10%까지 임의로 이동\n","    height_shift_range=0.1,\n","\n","    # 이미지를 시계 반대 방향으로 0.1 라디안 내외로 변형\n","    shear_range=0.1,\n","\n","    # 1에서 10% 줄이거나 늘리는 방식으로 이미지를 확대 또는 축소\n","    zoom_range=0.1,\n","\n","    # 이미지를 수평 방향으로 무작위로 뒤집음\n","    horizontal_flip=True,\n","\n","    # 이미지를 회전, 이동 또는 줌을 할 때 생기는 빈 픽셀을 인접한 픽셀로 채우는 방법을 지정\n","    fill_mode='nearest',\n","\n","    # 전체 데이터셋의 20%를 검증 데이터로 분할\n","    validation_split=0.2\n",")\n","\n","# 위와 같은 파라미터들을 사용하여 모델이 다양한 변형을 가진 이미지에서 작동할 수 있도록\n","# 훈련 데이터셋의 다양성을 증가시키고 데이터 증강은 과적합을 방지하며 모델의 일반화 능력을 향상시킬 수 있음.\n","\n","# ====================================================================================================\n","\n","# < 훈련 및 검증 데이터 생성기 (저장된 이미지 파일들을 신경망 모델에 공급할 준비) >\n","\n","# 주어진 경로에서 이미지를 로드하여 데이터 생성기를 생성\n","# 경로에는 여러 하위 폴더가 있고 각 하위 폴더의 이름은 클래스의 레이블로 사용\n","train_generator = train_datagen.flow_from_directory(\n","\n","    # 이미지 데이터가 저장된 디렉토리의 경로\n","    dataset_path,\n","\n","    # 모델에 입력될 이미지의 크기를 설정 (모든 이미지가 224x224 픽셀 크기로 조정)\n","    target_size=(img_width, img_height),\n","\n","    # 한 번에 모델로 전달될 이미지의 수를 설정 (한 배치에 32개)\n","    batch_size=batch_size,\n","\n","    # 분류 작업에 대한 레이블을 '원-핫 인코딩' 형식으로 생성하도록 설정\n","    class_mode='categorical',\n","\n","    # ImageDataGenerator에 정의된 validation_split 파라미터를 사용하여\n","    # 데이터셋을 훈련용과 검증용으로 나누는데 사용\n","    subset='training'# 훈련용\n",")\n","\n","# ====================================================================================================\n","\n","# < 모델의 성능을 검증하는 데 사용되는 이미지 배치를 생성 >\n","# train_generator와 동일한 옵션\n","validation_generator = train_datagen.flow_from_directory(\n","    dataset_path,\n","    target_size=(img_width, img_height),\n","    batch_size=batch_size,\n","    class_mode='categorical',\n","    subset='validation' # 검증용\n",")\n","\n","# ====================================================================================================\n","\n","# < ResNet50 모델 불러오기 및 사전 훈련된 가중치 사용 >\n","\n","# TensorFlow와 Keras 라이브러리를 사용하여 ResNet50 모델을 로드\n","# weights='imagenet': 모델을 ImageNet 데이터셋으로 사전 훈련된 가중치를 사용하여 초기화\n","# include_top=False: 네트워크의 최상위 레이어(일반적으로 완전 연결된 레이어)를 포함하지 않겠다는 의미\n","# input_shape=(img_width, img_height, 3): 네트워크에 공급될 입력 이미지의 shape을 설정\n","# img_width와 img_height는 이미지의 너비와 높이를 나타내고 3은 이미지의 채널 수(대개 RGB 이미지의 경우 3)를 의미\n","base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n","\n","# ====================================================================================================\n","\n","# < 모델 커스터마이징 >\n","\n","# base_model은 ResNet 모델을 의미\n","# base_model.output은 이 모델의 출력을 나타냄\n","# 출력은 추가적인 레이어에 연결될 기본 데이터임\n","x = base_model.output\n","\n","# 각 특징 맵(feature map)에 대해 평균값을 계산하여 더 적은 수의 특징으로 정보를 압축함\n","# 모델의 매개변수 수를 줄이고 과적합을 방지함\n","x = GlobalAveragePooling2D()(x)\n","\n","# 1024개의 뉴런을 가진 밀집(Dense) 레이어를 추가\n","# 활성화 함수로는 하이퍼볼릭 탄젠트(tanh)를 사용\n","# tanh 함수는 출력값을 -1과 1 사이\n","x = Dense(1024, activation='tanh')(x)\n","\n","# 드롭아웃 레이어를 추가하여 훈련 중 무작위로 50%(반절)의 뉴런을 비활성화\n","# 과적합을 방지\n","x = Dropout(0.5)(x)\n","\n","# 512, 256, 128개의 뉴런을 갖는 밀집 레이어를 추가\n","# tanh 활성화 함수 사용\n","x = Dense(512, activation='tanh')(x)\n","x = Dense(256, activation='tanh')(x)\n","x = Dense(128, activation='tanh')(x)\n","\n","# 최종 출력층 (분류해야 할 클래스의 수)\n","# softmax 활성화 함수는 다중 클래스 분류 문제에 적합함\n","# 각 클래스에 대한 확률 분포를 출력\n","predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n","\n","# ====================================================================================================\n","\n","# < 최종 모델 >\n","\n","# Model은 케라스에서 모델을 정의하는 데 사용되는 클래스\n","# 클래스를 사용하여 함수형 API를 통해 모델을 생성\n","# inputs=base_model.input: 모델의 입력으로 사용될 것을 정의\n","# base_model.input은 로드한 ResNet 모델의 입력을 의미\n","# outputs=predictions: 모델의 출력으로 사용될 것을 정의\n","# predictions는 신경망의 최종 출력 레이어 (활성화 함수인 softmax를 사용)\n","model = Model(inputs=base_model.input, outputs=predictions)\n","\n","# 위 코드는 사전 훈련된 ResNet 모델에 새로운 레이어를 추가하여\n","# 사용자 정의 모델을 만드는 과정이며 생성된 model 객체는 훈련, 평가, 예측 등에 사용\n","\n","# ====================================================================================================\n","\n","# < 모델 컴파일 > (Keras를 사용하여 신경망 모델을 컴파일하는 과정)\n","\n","# optimizer=Adam(learning_rate=0.0001): 모델의 최적화 알고리즘을 지정\n","# Adam 최적화 알고리즘을 사용\n","model.compile(optimizer=Adam(learning_rate=0.0001),\n","\n","              # 학습률(learning rate)을 0.0001로 설정\n","              loss='categorical_crossentropy',\n","\n","              # metrics=['accuracy']: 학습 과정에서 모니터링할 성능 지표를 지정\n","              # accuracy(정확도)를 사용\n","              metrics=['accuracy'])\n","\n","# 컴파일 단계를 통해 모델은 학습에 필요한 추가 설정을 완료하며\n","# 이후 모델의 fit 메서드를 통해 실제 학습을 시작\n","# 위 설정은 모델이 어떻게 학습할지와 학습 과정에서 어떻게 성능을 평가할지를 결정\n","\n","# ====================================================================================================\n","\n","# < 체크포인트 저장 경로 >\n","checkpoint_path = '/content/drive/MyDrive/Python_project/2023.12.26 프로젝트/KWAK/Cross_validation_checkpoint'\n","\n","# ====================================================================================================\n","\n","# < 체크포인트 콜백 설정 > (모델을 학습하는 동안 모델의 가중치를 자동으로 저장)\n","checkpoint_callback = ModelCheckpoint(\n","\n","    # checkpoint_path: 모델의 가중치를 저장할 파일 경로\n","    # 학습 중에 가장 좋은 모델의 가중치가 이 경로에 저장\n","    checkpoint_path,\n","\n","    # monitor='val_accuracy': 콜백이 모니터링할 성능 지표를 지정\n","    # 'val_accuracy'를 사용하여 검증 데이터셋에 대한 모델의 정확도를 기준으로 최적의 모델을 판단\n","    monitor='val_accuracy',\n","\n","    # verbose=1: 콜백의 진행 상황에 대한 자세한 출력을 활성화\n","    # 모델의 가중치가 갱신될 때마다 메시지를 출력\n","    verbose=1,\n","\n","    # save_best_only=True: 옵션을 True로 설정하면\n","    # 이전에 저장된 모델보다 더 나은 성능의 모델만 저장\n","    save_best_only=True,\n","\n","    # mode='max': monitor로 지정된 지표가 최대값일 때 모델을 저장\n","    # 'val_accuracy'와 같은 지표의 경우 높은 값이 더 좋은 성능을 의미하므로 'max' 모드를 사용\n","    mode='max'\n",")\n","\n","# 위 코드는 모델이 각 에폭마다 'val_accuracy'를 기준으로\n","# 현재까지의 최고 성능을 갱신할 때마다 해당 가중치를 checkpoint_path에 저장\n","# 학습이 끝난 후 최적의 성능을 낸 모델을 로드 할 수 있음\n","\n","# ====================================================================================================\n","\n","# < 콜백 설정 >\n","\n","# monitor='val_loss': 'val_loss' (검증 데이터셋에 대한 손실)를 모니터링\n","# patience=5: 모델이 5 에폭 동안 'val_loss'에서 개선되지 않으면 학습을 조기에 중단\n","# verbose=1: 진행 상황에 대한 자세한 정보를 출력\n","# mode='min': monitor로 지정된 지표가 최소값일 때 조치를 취하도록 지시하며\n","# 손실의 경우에는 낮은 값이 더 좋은 성능을 의미하므로 'min' 모드를 사용\n","early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min')\n","\n","# 콜백은 모델이 더 이상 개선되지 않을 때 과적합을 방지하기 위해 학습을 조기에 중단\n","# monitor='val_loss': 'val_loss'를 모니터링\n","# factor=0.2: 학습률을 감소시킬 때 적용할 감소율이고 현재 학습률에 이 값을 곱하여 새로운 학습률을 계산\n","# patience=2: 2 에폭 동안 'val_loss'에서 개선이 없을 경우 학습률을 감소\n","# verbose=1: 진행 상황에 대한 자세한 정보를 출력\n","# mode='min': monitor로 지정된 지표가 최소값일 때 조치를 취하도록 지시\n","# min_lr=0.00001: 학습률을 감소시킬 수 있는 최소값을 지정하여 학습률이 이 값 이하로 내려가지 않도록 해줌\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, verbose=1, mode='min', min_lr=0.00001)\n","\n","# 위 코드는 모델이 손실 함수의 국소 최소점에 빠지지 않고 전역 최소점에 더 가까워질 수 있게 해줌\n","\n","# ====================================================================================================\n","\n","# < 모델 학습 >\n","\n","# model.fit: 모델 학습 시작\n","history = model.fit(\n","\n","    # train_generator: 훈련 데이터를 제공하는 데이터 생성기이며 ImageDataGenerator를 사용하여\n","    # 생성된 train_generator는 디스크에서 이미지를 읽고 적절한 전처리를 수행한 후 모델에 공급\n","    train_generator,\n","\n","    # steps_per_epoch=len(train_generator): 한 에폭(epoch) 동안 훈련에 사용할 스텝(배치)의 수를 정의\n","    # train_generator에서 생성되는 전체 배치의 수와 동일하게 설정\n","    steps_per_epoch=len(train_generator),\n","\n","    # epochs=20: 모델이 훈련 데이터셋을 총 20번 반복해서 학습\n","    epochs=20,\n","\n","    # validation_data=validation_generator: 검증 데이터를 제공하는 데이터 생성기이며\n","    # 모델의 성능을 학습 중간에 평가하기 위해 사용\n","    validation_data=validation_generator,\n","\n","    # validation_steps=len(validation_generator): 한 에폭 동안 검증에 사용할 스텝(배치)의 수를 정의\n","    # validation_generator에서 생성되는 전체 배치의 수와 동일하게 설정\n","    validation_steps=len(validation_generator),\n","\n","    # callbacks=[checkpoint_callback, early_stopping, reduce_lr]: 콜백(callbacks) 리스트를 정의하고\n","    # 이전에 정의한 ModelCheckpoint, EarlyStopping, ReduceLROnPlateau 콜백이 포함\n","    callbacks=[checkpoint_callback, early_stopping, reduce_lr]\n",")\n","\n","# ====================================================================================================\n","\n","# < 모델 평가 >\n","\n","# model.evaluate 메서드를 사용하여 검증 데이터셋에 대한 모델의 손실(loss)과 정확도(accuracy)를 계산\n","\n","# model.evaluate: 모델을 평가하는 함수이며\n","# 주어진 데이터셋에 대해 모델의 손실과 메트릭(정확도)을 계산\n","\n","# validation_generator: 검증 데이터셋을 제공하는 데이터 생성기이며\n","# 검증 데이터셋의 이미지를 배치 단위로 모델에 공급\n","\n","# 반환값인 validation_loss와 validation_accuracy는\n","# 각각 검증 데이터셋에 대한 모델의 손실과 정확도를 나타냄\n","validation_loss, validation_accuracy = model.evaluate(validation_generator)\n","\n","# 검증 손실을 출력\n","print(f\"Validation Loss: {validation_loss}\")\n","\n","# 검증 정확도를 출력\n","print(f\"Validation Accuracy: {validation_accuracy}\")\n","\n","# ====================================================================================================\n","\n","# < 모델 저장 >\n","\n","# os.path.join: 여러 경로 구성 요소를 결합하여 하나의 경로를 형성 (확장자 = .h5)\n","model_save_path = os.path.join('/content/drive/MyDrive/Python_project/2023.12.26 프로젝트/KWAK/model_Res_Net', 'resnet_model.h5')\n","\n","# model.save: 전체 모델(아키텍처, 가중치, 훈련 구성, 옵티마이저 상태 등)을\n","# 단일 파일에 저장하여 지정 경로에 모델 파일을 생성\n","model.save(model_save_path)\n","\n","# ====================================================================================================\n","\n","# 교차 검증을 위한 EfficientNetB0 모델 사용\n","# Res_Net과 동일한 환경과 튜닝 값으로 학습을 진행\n","# Res_Net과 동일한 코드 (설명 생략)\n","\n","# < EfficientNetB0 모델 로드 >\n","base_model_eff = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n","\n","# < 모델 커스터마이징 >\n","x = base_model_eff.output\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(1024, activation='tanh')(x)\n","x = Dropout(0.5)(x)\n","x = Dense(512, activation='tanh')(x)  # 탄젠트 활성화 함수를 사용하는 추가 밀집 레이어\n","x = Dense(256, activation='tanh')(x)  # 추가 밀집 레이어\n","x = Dense(128, activation='tanh')(x)  # 추가 밀집 레이어\n","predictions = Dense(train_generator.num_classes, activation='softmax')(x)  # 출력층\n","\n","# < 최종 모델 >\n","model_eff = Model(inputs=base_model_eff.input, outputs=predictions)\n","\n","# < 모델 컴파일 >\n","model_eff.compile(optimizer=Adam(learning_rate=0.0001),\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","# < 모델 학습 >\n","history_eff = model_eff.fit(\n","    train_generator,\n","    steps_per_epoch=len(train_generator),\n","    epochs=20,\n","    validation_data=validation_generator,\n","    validation_steps=len(validation_generator),\n","    callbacks=[checkpoint_callback, early_stopping, reduce_lr]\n",")\n","\n","# < 모델 저장 >\n","model_save_path_eff = os.path.join('/content/drive/MyDrive/Python_project/2023.12.26 프로젝트/KWAK/model_EfficientNet', 'efficientnet_model.h5')\n","model_eff.save(model_save_path_eff)\n"]},{"cell_type":"markdown","source":["# 결과출력 및 시각화"],"metadata":{"id":"xVreKblv7-bW"}},{"cell_type":"code","source":["# < 라이브러리 임포트 >\n","import numpy as np\n","import seaborn as sns\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"],"metadata":{"id":"YBr9fRiC5LG1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# < 테스트 데이터셋에 대한 데이터 생성기(test_generator)를 설정 >\n","\n","# 테스트 데이터셋이 저장된 디렉토리의 경로\n","test_dataset_path = '/content/drive/MyDrive/Python_project/2023.12.26 프로젝트/data/test_dataset_Overseas'\n","\n","# 모든 이미지 픽셀 값을 [0, 255] 범위에서 [0, 1] 범위로 조정 (스케일링)\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","# 디스크의 디렉토리로부터 이미지를 읽어 데이터 생성기를 설정\n","test_generator = test_datagen.flow_from_directory(\n","\n","    # 이미지를 로드할 디렉토리의 경로\n","    test_dataset_path,\n","\n","    # 모든 이미지를 224x224 픽셀 크기로 조정\n","    target_size=(224, 224),\n","\n","    # 한 번에 모델로 전달될 이미지의 수를 32로 설정\n","    batch_size=32,\n","\n","    # 다중 클래스 분류를 위해 레이블을 '원-핫 인코딩' 형태로 변환\n","    class_mode='categorical',\n","\n","    # 테스트 데이터셋의 순서를 유지 (셔플을 사용하지 않음)\n","    shuffle=False\n",")\n"],"metadata":{"id":"gbvzfglE5MMN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# < 저장된 모델 로드 >\n","\n","resnet_model_path = '/content/drive/MyDrive/Python_project/2023.12.26 프로젝트/KWAK/model_Res_Net/resnet_model.h5'\n","efficientnet_model_path = '/content/drive/MyDrive/Python_project/2023.12.26 프로젝트/KWAK/model_EfficientNet/efficientnet_model.h5'\n","\n","resnet_model = load_model(resnet_model_path)\n","efficientnet_model = load_model(efficientnet_model_path)\n"],"metadata":{"id":"4i8Bd1He5NR9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 평가\n","resnet_results = resnet_model.evaluate(test_generator, verbose=0)\n","efficientnet_results = efficientnet_model.evaluate(test_generator, verbose=0)\n","\n","# 정확도 출력\n","print(f'ResNet Test Accuracy: {resnet_results[1]}')\n","print(f'EfficientNet Test Accuracy: {efficientnet_results[1]}')\n","\n","# 예측 및 실제 레이블 비교\n","test_generator.reset()  # 인덱스 리셋\n","resnet_predictions = np.argmax(resnet_model.predict(test_generator), axis=1)\n","test_generator.reset()  # 다시 리셋\n","efficientnet_predictions = np.argmax(efficientnet_model.predict(test_generator), axis=1)\n","true_labels = test_generator.classes\n","\n","# 오분류된 데이터 분석\n","misclassified_resnet = np.where(resnet_predictions != true_labels)[0]\n","misclassified_efficientnet = np.where(efficientnet_predictions != true_labels)[0]\n","\n","# 오분류된 데이터 인덱스 출력\n","print(f'Misclassified by ResNet: {misclassified_resnet}')\n","print(f'Misclassified by EfficientNet: {misclassified_efficientnet}')\n"],"metadata":{"id":"W2nTwDDW5Qnl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 테스트 데이터셋에 대한 모델의 예측\n","y_pred_resnet = resnet_model.predict(test_generator)\n","y_pred_efficientnet = efficientnet_model.predict(test_generator)\n","\n","# 실제 클래스 레이블\n","true_classes = test_generator.classes\n","\n","# 예측된 클래스 레이블\n","predicted_classes_resnet = np.argmax(y_pred_resnet, axis=1)\n","predicted_classes_efficientnet = np.argmax(y_pred_efficientnet, axis=1)\n","\n","# 혼동 행렬 계산\n","cm_resnet = confusion_matrix(true_classes, predicted_classes_resnet)\n","cm_efficientnet = confusion_matrix(true_classes, predicted_classes_efficientnet)\n"],"metadata":{"id":"6xXTaE_977Nd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# < matplotlib 한글화 >\n","\n","import matplotlib.pyplot as plt\n","!apt-get update -qq\n","!apt-get install fonts-nanum* -qq\n","import matplotlib.font_manager as fm\n","fe = fm.FontEntry(\n","    fname=r'/usr/share/fonts/truetype/nanum/NanumGothic.ttf',  name='NanumGothic')\n","fm.fontManager.ttflist.insert(0, fe)\n","plt.rcParams.update({'font.size': 18, 'font.family': 'NanumGothic'})"],"metadata":{"id":"BC4EAAyjASfW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 클래스 레이블 목록\n","class_labels = list(test_generator.class_indices.keys())\n","\n","# ResNet 모델 혼동 행렬 시각화\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(cm_resnet, annot=True, cmap='Reds', fmt='g', xticklabels=class_labels, yticklabels=class_labels)\n","plt.title('ResNet Model')\n","plt.xlabel('예측값')\n","plt.ylabel('실제값')\n","plt.show()\n","\n","# EfficientNet 모델 혼동 행렬 시각화\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(cm_efficientnet, annot=True, cmap='Blues', fmt='g', xticklabels=class_labels, yticklabels=class_labels)\n","plt.title('EfficientNet Model')\n","plt.xlabel('예측값')\n","plt.ylabel('실제값')\n","plt.show()\n"],"metadata":{"id":"3QrsskDC78id"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 훈련 이력에서 'val_loss' 데이터를 로드\n","# 에포크 1 ~ 13 까지의 val_loss\n","resnet_val_loss = history.history['val_loss'][:14]\n","efficientnet_val_loss = history_eff.history['val_loss'][:14]\n","\n","# 실제 손실 값의 길이에 맞는 에포크 범위를 설정\n","epochs_trimmed_resnet = range(1, len(resnet_val_loss) + 1)\n","epochs_trimmed_efficientnet = range(1, len(efficientnet_val_loss) + 1)\n","\n","# 두 모델의 검증 손실을 플로팅(시각화)\n","plt.figure(figsize=(10, 6))\n","plt.plot(epochs_trimmed_resnet, resnet_val_loss, 'r', marker='o', label='ResNet')\n","plt.plot(epochs_trimmed_efficientnet, efficientnet_val_loss, 'b', marker='o', label='EfficientNetB0')\n","plt.title('검증 손실률 비교')\n","plt.xlabel('Epoch')\n","plt.xticks(range(1, 15))\n","plt.ylabel('loss')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"vHrTAKZ_8nqO"},"execution_count":null,"outputs":[]}]}