{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46398,"status":"ok","timestamp":1703565722752,"user":{"displayName":"JW P","userId":"05164750983755444081"},"user_tz":-540},"id":"7gEVYFxbz24O","outputId":"926aa332-505d-4d31-b308-828ef5c1b50c"},"outputs":[{"name":"stdout","output_type":"stream","text":["glass: 1420 images\n","metal: 1614 images\n","paper: 1640 images\n","vinyl: 1362 images\n","plastic/label: 612 images\n","plastic/no_label: 543 images\n"]}],"source":["# 파일 갯수 확인\n","import os\n","dataset_path = '/content/drive/MyDrive/00_05_4_daejeon_3/2023.12.26 프로젝트/data/current_dataset'\n","\n","# 쓰레기 카테고리\n","categories = ['glass', 'metal', 'paper', 'vinyl']\n","subcategories = {'plastic': ['label', 'no_label']}\n","\n","def count_images_in_folder(folder_path):\n","    \"\"\" 해당 폴더 내의 이미지 파일 수를 세는 함수 \"\"\"\n","    return len([file for file in os.listdir(folder_path) if file.lower().endswith(('.png', '.jpg', '.jpeg'))])\n","\n","# 메인 카테고리별 이미지 수 계산\n","for category in categories:\n","    category_path = os.path.join(dataset_path, category)\n","    if os.path.isdir(category_path):\n","        count = count_images_in_folder(category_path)\n","        print(f\"{category}: {count} images\")\n","\n","# 플라스틱 서브카테고리별 이미지 수 계산\n","for subcategory in subcategories['plastic']:\n","    subcategory_path = os.path.join(dataset_path, 'plastic', subcategory)\n","    if os.path.isdir(subcategory_path):\n","        count = count_images_in_folder(subcategory_path)\n","        print(f\"plastic/{subcategory}: {count} images\")"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5294,"status":"ok","timestamp":1703565728042,"user":{"displayName":"JW P","userId":"05164750983755444081"},"user_tz":-540},"id":"aPCNvQTC6Gyg","outputId":"ace80181-ecb1-4524-a20a-5981efc4f5fe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting keras-tuner\n","  Downloading keras_tuner-1.4.6-py3-none-any.whl (128 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/128.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/128.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.9/128.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n","Collecting kt-legacy (from keras-tuner)\n","  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ekeras-tuner) (3.3.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ekeras-tuner) (3.6)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ekeras-tuner) (2.0.7)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ekeras-tuner) (2023.11.17)\n","Installing collected packages: kt-legacy, keras-tuner\n","Successfully installed keras-tuner-1.4.6 kt-legacy-1.0.5\n"]}],"source":["!pip install keras-tuner"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24138,"status":"ok","timestamp":1703565752177,"user":{"displayName":"JW P","userId":"05164750983755444081"},"user_tz":-540},"id":"bDXGpyTn1NFu","outputId":"070c89b8-211c-41f6-ba0d-837fb86b30a8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 5755 images belonging to 5 classes.\n"]},{"name":"stderr","output_type":"stream","text":["\u003cipython-input-3-c434948f9a0e\u003e:18: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n","  from kerastuner import HyperParameters as hp\n"]},{"name":"stdout","output_type":"stream","text":["Found 1437 images belonging to 5 classes.\n","Reloading Tuner from /content/drive/MyDrive/00_05_4_daejeon_3/2023.12.26 프로젝트/HYUN/hpt/ResNet50_hpt_2/tuner0.json\n","\n","Best Hyperparameters:\n","{'batch_size': 128, 'units': 640, 'dense_activation': 'tanh', 'dropout_rate': 0.2, 'learning_rate': 0.00694090003577775, 'optimizer': 'sgd'}\n","\n","Trial ID: 04 does not have 'val_accuracy' metric recorded.\n","Trial ID: 1 does not have 'val_accuracy' metric recorded.\n","Trial ID: 0, Best Validation Accuracy: 0.8162475824356079\n","Trial ID: 05, Best Validation Accuracy: 0.7193208734194437\n","Trial ID: 3 does not have 'val_accuracy' metric recorded.\n","Trial ID: 07, Best Validation Accuracy: 0.8953363498051962\n","Trial ID: 06, Best Validation Accuracy: 0.4294003943602244\n","Trial ID: 2, Best Validation Accuracy: 0.8149580955505371\n","Trial ID: 10, Best Validation Accuracy: 0.7491940855979919\n","Trial ID: 09, Best Validation Accuracy: 0.7790672779083252\n","Trial ID: 13, Best Validation Accuracy: 0.8736299276351929\n","Trial ID: 12, Best Validation Accuracy: 0.6690307259559631\n","Trial ID: 14, Best Validation Accuracy: 0.6140124797821045\n","Trial ID: 08, Best Validation Accuracy: 0.38512787222862244\n","Trial ID: 11, Best Validation Accuracy: 0.8456909656524658\n"]}],"source":["\n","\n","import tensorflow as tf\n","from tensorflow.keras.applications import ResNet50\n","from tensorflow.keras.optimizers import Adam\n","import os\n","from tensorflow import keras\n","from tensorflow.keras.layers import Dense, Dropout, Input\n","from tensorflow.keras.models import Model\n","import keras_tuner\n","from keras_tuner import RandomSearch\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.models import load_model\n","import cv2\n","import numpy as np\n","from tensorflow.keras.applications import EfficientNetB3\n","from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n","from tensorflow.keras.models import Model\n","from kerastuner import HyperParameters as hp\n","import glob\n","from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n","\n","\n","\n","# 데이터셋 경로\n","dataset_path = '/content/drive/MyDrive/00_05_4_daejeon_3/2023.12.26 프로젝트/data/current_dataset'\n","\n","# 이미지 크기 및 배치 크기 설정\n","img_width, img_height = 224, 224  # ResNet50의 기본 이미지 크기\n","\n","\n","# 모델 생성\n","def build_model(hp):\n","    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n","\n","    x = base_model.output\n","    x = GlobalAveragePooling2D()(x)\n","    x = Dense(units=hp.Int('units', min_value=512, max_value=1024, step=32),\n","              activation=hp.Choice('dense_activation', values=['relu', 'tanh', 'sigmoid']))(x)\n","    x = Dropout(rate=hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1))(x)\n","    predictions = Dense(5, activation='softmax')(x)\n","\n","    # 학습률 및 옵티마이저 튜닝\n","    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n","    optimizer_choice = hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop'])\n","\n","    if optimizer_choice == 'adam':\n","        optimizer = Adam(learning_rate=learning_rate)\n","    elif optimizer_choice == 'sgd':\n","        optimizer = SGD(learning_rate=learning_rate)\n","    else:\n","        optimizer = RMSprop(learning_rate=learning_rate)\n","\n","    model = Model(inputs=base_model.input, outputs=predictions)\n","\n","    model.compile(optimizer=optimizer,\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    return model\n","\n","\n","# 배치사이즈 / ImageDataGenerator / train,val 분류\n","def create_generators(hp):\n","    batch_size = hp.Int('batch_size', min_value=32, max_value=128, step=32)\n","\n","    train_datagen = ImageDataGenerator(\n","        rescale=1./255,\n","        rotation_range=10,\n","        width_shift_range=0.1,\n","        height_shift_range=0.1,\n","        shear_range=0.1,\n","        zoom_range=0.1,\n","        horizontal_flip=True,\n","        fill_mode='nearest',\n","        validation_split=0.2\n","    )\n","\n","    train_generator = train_datagen.flow_from_directory(\n","        dataset_path,\n","        target_size=(img_width, img_height),\n","        batch_size=batch_size,\n","        class_mode='categorical',\n","        subset='training'\n","    )\n","\n","    validation_generator = train_datagen.flow_from_directory(\n","        dataset_path,\n","        target_size=(img_width, img_height),\n","        batch_size=batch_size,\n","        class_mode='categorical',\n","        subset='validation'\n","    )\n","\n","    return train_generator, validation_generator\n","\n","# 하이퍼파라미터 및 튜너 설정\n","hyperparameters = keras_tuner.HyperParameters()\n","\n","# 데이터 생성기 생성\n","train_generator, validation_generator = create_generators(hyperparameters)\n","\n","\n","# 튜너 설정\n","tuner = RandomSearch(\n","    build_model,\n","    objective='val_accuracy',\n","    max_trials=15,  # 시도할 최대 트라이얼 수\n","    executions_per_trial=3,  # 각 트라이얼마다의 실행 횟수\n","    # 튜닝 결과 저장 경로\n","    directory='/content/drive/MyDrive/00_05_4_daejeon_3/2023.12.26 프로젝트/HYUN/hpt',\n","    # 폴더 이름\n","    project_name='ResNet50_hpt_2',\n","    hyperparameters=hyperparameters,\n","    overwrite=False  # 이전 튜닝 세션을 재개하기 위해 False로 설정\n",")\n","\n","# 튜너의 결과를 로드합니다.\n","tuner.reload()\n","\n","# 최적의 하이퍼파라미터를 가져옵니다.\n","best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n","\n","print('\\nBest Hyperparameters:')\n","print(best_hps.values,end='\\n\\n')\n","\n","\n","\n","for trial in tuner.oracle.trials.values():\n","    # 메트릭 중 'val_accuracy'가 존재하는지 확인\n","    if 'val_accuracy' in trial.metrics.metrics:\n","        val_accuracy = trial.metrics.get_best_value('val_accuracy')\n","        print(f'Trial ID: {trial.trial_id}, Best Validation Accuracy: {val_accuracy}')\n","    else:\n","        # 'val_accuracy'가 없는 경우, 대체 메시지 출력\n","        print(f'Trial ID: {trial.trial_id} does not have \\'val_accuracy\\' metric recorded.')\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1703565752178,"user":{"displayName":"JW P","userId":"05164750983755444081"},"user_tz":-540},"id":"qpxmIBg1shXT","outputId":"5bdb53c0-b4ed-4e77-ddbc-662c82d63cd3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Trials sorted by Best Validation Accuracy:\n","Trial ID: 07, Best Validation Accuracy: 0.8953363498051962\n","Hyperparameters:\n","  batch_size: 128\n","  units: 640\n","  dense_activation: tanh\n","  dropout_rate: 0.2\n","  learning_rate: 0.00694090003577775\n","  optimizer: sgd\n","\n","Trial ID: 13, Best Validation Accuracy: 0.8736299276351929\n","Hyperparameters:\n","  batch_size: 64\n","  units: 832\n","  dense_activation: tanh\n","  dropout_rate: 0.2\n","  learning_rate: 0.0031468652789622942\n","  optimizer: sgd\n","\n","Trial ID: 11, Best Validation Accuracy: 0.8456909656524658\n","Hyperparameters:\n","  batch_size: 96\n","  units: 928\n","  dense_activation: relu\n","  dropout_rate: 0.1\n","  learning_rate: 0.00019480904540985537\n","  optimizer: adam\n","\n","Trial ID: 0, Best Validation Accuracy: 0.8162475824356079\n","Hyperparameters:\n","  batch_size: 32\n","  units: 512\n","  dense_activation: tanh\n","  dropout_rate: 0.0\n","  learning_rate: 0.00024124275097430977\n","  optimizer: adam\n","\n","Trial ID: 2, Best Validation Accuracy: 0.8149580955505371\n","Hyperparameters:\n","  batch_size: 96\n","  units: 800\n","  dense_activation: tanh\n","  dropout_rate: 0.30000000000000004\n","  learning_rate: 0.00023934916758165718\n","  optimizer: adam\n","\n","Trial ID: 09, Best Validation Accuracy: 0.7790672779083252\n","Hyperparameters:\n","  batch_size: 64\n","  units: 640\n","  dense_activation: relu\n","  dropout_rate: 0.0\n","  learning_rate: 0.0003914299502088062\n","  optimizer: adam\n","\n","Trial ID: 10, Best Validation Accuracy: 0.7491940855979919\n","Hyperparameters:\n","  batch_size: 32\n","  units: 960\n","  dense_activation: tanh\n","  dropout_rate: 0.1\n","  learning_rate: 0.0006023443262491292\n","  optimizer: rmsprop\n","\n","Trial ID: 05, Best Validation Accuracy: 0.7193208734194437\n","Hyperparameters:\n","  batch_size: 128\n","  units: 960\n","  dense_activation: tanh\n","  dropout_rate: 0.2\n","  learning_rate: 0.00018681519336132603\n","  optimizer: sgd\n","\n","Trial ID: 12, Best Validation Accuracy: 0.6690307259559631\n","Hyperparameters:\n","  batch_size: 96\n","  units: 576\n","  dense_activation: tanh\n","  dropout_rate: 0.1\n","  learning_rate: 0.0006917829860392126\n","  optimizer: adam\n","\n","Trial ID: 14, Best Validation Accuracy: 0.6140124797821045\n","Hyperparameters:\n","  batch_size: 96\n","  units: 832\n","  dense_activation: relu\n","  dropout_rate: 0.2\n","  learning_rate: 0.0013369436103508122\n","  optimizer: adam\n","\n","Trial ID: 06, Best Validation Accuracy: 0.4294003943602244\n","Hyperparameters:\n","  batch_size: 96\n","  units: 704\n","  dense_activation: tanh\n","  dropout_rate: 0.0\n","  learning_rate: 0.0012502824673384133\n","  optimizer: adam\n","\n","Trial ID: 08, Best Validation Accuracy: 0.38512787222862244\n","Hyperparameters:\n","  batch_size: 32\n","  units: 640\n","  dense_activation: sigmoid\n","  dropout_rate: 0.0\n","  learning_rate: 0.005799986951973708\n","  optimizer: adam\n","\n"]}],"source":["# 트라이얼 데이터와 val_accuracy를 추출하고 정렬\n","trials_data = []\n","for trial in tuner.oracle.trials.values():\n","    if 'val_accuracy' in trial.metrics.metrics:\n","        val_accuracy = trial.metrics.get_best_value('val_accuracy')\n","        trials_data.append((trial.trial_id, val_accuracy, trial.hyperparameters.values))\n","\n","# val_accuracy 기준으로 내림차순 정렬\n","trials_data.sort(key=lambda x: x[1], reverse=True)\n","\n","# 정렬된 결과 출력\n","print(\"Trials sorted by Best Validation Accuracy:\")\n","for trial_id, val_accuracy, hyperparameters in trials_data:\n","    print(f\"Trial ID: {trial_id}, Best Validation Accuracy: {val_accuracy}\")\n","    print(\"Hyperparameters:\")\n","    for param, value in hyperparameters.items():\n","        print(f\"  {param}: {value}\")\n","    print()\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1703565752178,"user":{"displayName":"JW P","userId":"05164750983755444081"},"user_tz":-540},"id":"qrZHt3QkajDo","outputId":"a745086e-c095-48f3-fe6c-9b17b957b61a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Second Best Trial ID: 13, Validation Accuracy: 0.8736299276351929\n"]}],"source":["# 모든 시도의 성능을 저장할 리스트를 초기화\n","trials_performance = []\n","\n","for trial in tuner.oracle.trials.values():\n","    # 각 시도의 'val_accuracy'를 가져옵니다.\n","    if 'val_accuracy' in trial.metrics.metrics:\n","        val_accuracy = trial.metrics.get_best_value('val_accuracy')\n","        trials_performance.append((trial.trial_id, val_accuracy))\n","\n","# 성능에 따라 시도들을 정렬합니다 (내림차순).\n","trials_performance.sort(key=lambda x: x[1], reverse=True)\n","\n","# 두 번째로 높은 성능을 가진 시도의 정보를 출력합니다.\n","if len(trials_performance) \u003e 1:\n","    second_best_trial_id, second_best_val_accuracy = trials_performance[1]\n","    print(f\"Second Best Trial ID: {second_best_trial_id}, Validation Accuracy: {second_best_val_accuracy}\")\n","else:\n","    print(\"Not enough trials to determine the second best.\")\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1703565752178,"user":{"displayName":"JW P","userId":"05164750983755444081"},"user_tz":-540},"id":"7XslFkG5s6c-","outputId":"479b1a12-3abe-4055-8e05-c8c495c4c0e2"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'batch_size': 64, 'units': 832, 'dense_activation': 'tanh', 'dropout_rate': 0.2, 'learning_rate': 0.0031468652789622942, 'optimizer': 'sgd'}\n","\n"]}],"source":["# Trial ID: 13, Best Validation Accuracy: 0.8736299276351929\n","# Hyperparameters:\n","#   batch_size: 64\n","#   units: 832\n","#   dense_activation: tanh\n","#   dropout_rate: 0.2\n","#   learning_rate: 0.0031468652789622942\n","#   optimizer: sgd\n","best_hps = tuner.get_best_hyperparameters(num_trials=2)[1]\n","print(best_hps.values,end='\\n\\n')"]},{"cell_type":"markdown","metadata":{"id":"2dvCQm4bslVl"},"source":["# 최적의 튜닝으로 이어서 학습"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6430978,"status":"ok","timestamp":1703572183145,"user":{"displayName":"JW P","userId":"05164750983755444081"},"user_tz":-540},"id":"R6Ql9NdErMsB","outputId":"e17fdbda-3f6e-4979-c500-c2df4ecf3d8e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 5755 images belonging to 5 classes.\n","Found 1437 images belonging to 5 classes.\n","Reloading Tuner from /content/drive/MyDrive/00_05_4_daejeon_3/2023.12.26 프로젝트/HYUN/hpt/ResNet50_hpt_2/tuner0.json\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n","94765736/94765736 [==============================] - 5s 0us/step\n","Epoch 1/100\n","180/180 [==============================] - ETA: 0s - loss: 0.6311 - accuracy: 0.7804 "]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 1: val_accuracy improved from -inf to 0.22408, saving model to /content/drive/MyDrive/00_05_4_daejeon_3/2023.12.26 프로젝트/HYUN/model/best_restnet4/ResNet50_01-0.2241.hdf5\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"name":"stdout","output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r180/180 [==============================] - 4486s 25s/step - loss: 0.6311 - accuracy: 0.7804 - val_loss: 1.9144 - val_accuracy: 0.2241\n","Epoch 2/100\n","180/180 [==============================] - ETA: 0s - loss: 0.2552 - accuracy: 0.9142\n","Epoch 2: val_accuracy improved from 0.22408 to 0.23452, saving model to /content/drive/MyDrive/00_05_4_daejeon_3/2023.12.26 프로젝트/HYUN/model/best_restnet4/ResNet50_02-0.2345.hdf5\n","180/180 [==============================] - 108s 596ms/step - loss: 0.2552 - accuracy: 0.9142 - val_loss: 2.1029 - val_accuracy: 0.2345\n","Epoch 3/100\n","180/180 [==============================] - ETA: 0s - loss: 0.1718 - accuracy: 0.9434\n","Epoch 3: val_accuracy improved from 0.23452 to 0.37300, saving model to /content/drive/MyDrive/00_05_4_daejeon_3/2023.12.26 프로젝트/HYUN/model/best_restnet4/ResNet50_03-0.3730.hdf5\n","180/180 [==============================] - 106s 585ms/step - loss: 0.1718 - accuracy: 0.9434 - val_loss: 1.8340 - val_accuracy: 0.3730\n","Epoch 4/100\n","180/180 [==============================] - ETA: 0s - loss: 0.1344 - accuracy: 0.9585\n","Epoch 4: val_accuracy improved from 0.37300 to 0.54767, saving model to /content/drive/MyDrive/00_05_4_daejeon_3/2023.12.26 프로젝트/HYUN/model/best_restnet4/ResNet50_04-0.5477.hdf5\n","180/180 [==============================] - 105s 584ms/step - loss: 0.1344 - accuracy: 0.9585 - val_loss: 1.6122 - val_accuracy: 0.5477\n","Epoch 5/100\n","180/180 [==============================] - ETA: 0s - loss: 0.1027 - accuracy: 0.9672\n","Epoch 5: val_accuracy improved from 0.54767 to 0.67363, saving model to /content/drive/MyDrive/00_05_4_daejeon_3/2023.12.26 프로젝트/HYUN/model/best_restnet4/ResNet50_05-0.6736.hdf5\n","180/180 [==============================] - 105s 583ms/step - loss: 0.1027 - accuracy: 0.9672 - val_loss: 1.0293 - val_accuracy: 0.6736\n","Epoch 6/100\n","180/180 [==============================] - ETA: 0s - loss: 0.0835 - accuracy: 0.9738\n","Epoch 6: val_accuracy improved from 0.67363 to 0.80863, saving model to /content/drive/MyDrive/00_05_4_daejeon_3/2023.12.26 프로젝트/HYUN/model/best_restnet4/ResNet50_06-0.8086.hdf5\n","180/180 [==============================] - 105s 583ms/step - loss: 0.0835 - accuracy: 0.9738 - val_loss: 0.5634 - val_accuracy: 0.8086\n","Epoch 7/100\n","180/180 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9772\n","Epoch 7: val_accuracy improved from 0.80863 to 0.85108, saving model to /content/drive/MyDrive/00_05_4_daejeon_3/2023.12.26 프로젝트/HYUN/model/best_restnet4/ResNet50_07-0.8511.hdf5\n","180/180 [==============================] - 106s 586ms/step - loss: 0.0697 - accuracy: 0.9772 - val_loss: 0.4416 - val_accuracy: 0.8511\n","Epoch 8/100\n","180/180 [==============================] - ETA: 0s - loss: 0.0549 - accuracy: 0.9840\n","Epoch 8: val_accuracy improved from 0.85108 to 0.87056, saving model to /content/drive/MyDrive/00_05_4_daejeon_3/2023.12.26 프로젝트/HYUN/model/best_restnet4/ResNet50_08-0.8706.hdf5\n","180/180 [==============================] - 105s 584ms/step - loss: 0.0549 - accuracy: 0.9840 - val_loss: 0.4056 - val_accuracy: 0.8706\n","Epoch 9/100\n","180/180 [==============================] - ETA: 0s - loss: 0.0453 - accuracy: 0.9880\n","Epoch 9: val_accuracy improved from 0.87056 to 0.88587, saving model to /content/drive/MyDrive/00_05_4_daejeon_3/2023.12.26 프로젝트/HYUN/model/best_restnet4/ResNet50_09-0.8859.hdf5\n","180/180 [==============================] - 104s 578ms/step - loss: 0.0453 - accuracy: 0.9880 - val_loss: 0.3451 - val_accuracy: 0.8859\n","Epoch 10/100\n","180/180 [==============================] - ETA: 0s - loss: 0.0437 - accuracy: 0.9887\n","Epoch 10: val_accuracy did not improve from 0.88587\n","180/180 [==============================] - 105s 582ms/step - loss: 0.0437 - accuracy: 0.9887 - val_loss: 0.4029 - val_accuracy: 0.8678\n","Epoch 11/100\n","180/180 [==============================] - ETA: 0s - loss: 0.0390 - accuracy: 0.9882\n","Epoch 11: val_accuracy did not improve from 0.88587\n","180/180 [==============================] - 105s 580ms/step - loss: 0.0390 - accuracy: 0.9882 - val_loss: 0.3897 - val_accuracy: 0.8775\n","Epoch 12/100\n","180/180 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9939\n","Epoch 12: val_accuracy did not improve from 0.88587\n","180/180 [==============================] - 104s 576ms/step - loss: 0.0285 - accuracy: 0.9939 - val_loss: 0.4122 - val_accuracy: 0.8754\n","Epoch 13/100\n","180/180 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9953\n","Epoch 13: val_accuracy did not improve from 0.88587\n","180/180 [==============================] - 105s 583ms/step - loss: 0.0259 - accuracy: 0.9953 - val_loss: 0.4261 - val_accuracy: 0.8706\n","Epoch 14/100\n","180/180 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9948\n","Epoch 14: val_accuracy did not improve from 0.88587\n","180/180 [==============================] - 105s 580ms/step - loss: 0.0235 - accuracy: 0.9948 - val_loss: 0.3929 - val_accuracy: 0.8810\n","Epoch 15/100\n","180/180 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9964\n","Epoch 15: val_accuracy did not improve from 0.88587\n","180/180 [==============================] - 104s 579ms/step - loss: 0.0196 - accuracy: 0.9964 - val_loss: 0.4382 - val_accuracy: 0.8720\n","Epoch 16/100\n","180/180 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9936\n","Epoch 16: val_accuracy did not improve from 0.88587\n","180/180 [==============================] - 104s 579ms/step - loss: 0.0246 - accuracy: 0.9936 - val_loss: 0.4182 - val_accuracy: 0.8754\n","Epoch 17/100\n","180/180 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9962\n","Epoch 17: val_accuracy did not improve from 0.88587\n","180/180 [==============================] - 104s 575ms/step - loss: 0.0186 - accuracy: 0.9962 - val_loss: 0.4326 - val_accuracy: 0.8754\n","Epoch 18/100\n","180/180 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9962\n","Epoch 18: val_accuracy did not improve from 0.88587\n","180/180 [==============================] - 104s 576ms/step - loss: 0.0164 - accuracy: 0.9962 - val_loss: 0.3990 - val_accuracy: 0.8859\n","Epoch 19/100\n","180/180 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9976\n","Epoch 19: val_accuracy did not improve from 0.88587\n","180/180 [==============================] - 105s 584ms/step - loss: 0.0156 - accuracy: 0.9976 - val_loss: 0.4609 - val_accuracy: 0.8740\n","Epoch 19: early stopping\n","45/45 [==============================] - 21s 458ms/step - loss: 0.4747 - accuracy: 0.8775\n","Validation Loss: 0.4747288227081299\n","Validation Accuracy: 0.8775225877761841\n"]}],"source":["# 최적의 튜닝으로 학습 진행\n","\n","import tensorflow as tf\n","from tensorflow.keras.applications import ResNet50\n","import os\n","from tensorflow import keras\n","from tensorflow.keras.layers import Dense, Dropout, Input\n","import keras_tuner\n","from keras_tuner import RandomSearch\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.models import load_model\n","import cv2\n","import numpy as np\n","from tensorflow.keras.applications import EfficientNetB3\n","from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n","from tensorflow.keras.models import Model\n","from kerastuner import HyperParameters as hp\n","import glob\n","from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n","from PIL import Image\n","from tensorflow.keras.preprocessing import image\n","\n","\n","# 데이터셋 경로\n","dataset_path = '/content/drive/MyDrive/00_05_4_daejeon_3/2023.12.26 프로젝트/data/current_dataset'\n","\n","# 이미지 크기 및 배치 크기 설정\n","img_width, img_height = 224, 224  # ResNet50의 기본 이미지 크기\n","\n","# 모델 생성\n","def build_model(hp):\n","    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n","\n","    x = base_model.output\n","    x = GlobalAveragePooling2D()(x)\n","    x = Dense(units=hp.Int('units', min_value=512, max_value=1024, step=32),\n","              activation=hp.Choice('dense_activation', values=['relu', 'tanh', 'sigmoid']))(x)\n","    x = Dropout(rate=hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1))(x)\n","    predictions = Dense(5, activation='softmax')(x)\n","\n","    # 학습률 및 옵티마이저 튜닝\n","    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n","    optimizer_choice = hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop'])\n","\n","    if optimizer_choice == 'adam':\n","        optimizer = Adam(learning_rate=learning_rate)\n","    elif optimizer_choice == 'sgd':\n","        optimizer = SGD(learning_rate=learning_rate)\n","    else:\n","        optimizer = RMSprop(learning_rate=learning_rate)\n","\n","    model = Model(inputs=base_model.input, outputs=predictions)\n","\n","    model.compile(optimizer=optimizer,\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    return model\n","\n","\n","# 배치사이즈 / ImageDataGenerator / train,val 분류\n","def create_generators(hp):\n","    batch_size = hp.Int('batch_size', min_value=32, max_value=128, step=32)\n","\n","    train_datagen = ImageDataGenerator(\n","        rescale=1./255,\n","        rotation_range=10,\n","        width_shift_range=0.1,\n","        height_shift_range=0.1,\n","        shear_range=0.1,\n","        zoom_range=0.1,\n","        horizontal_flip=True,\n","        fill_mode='nearest',\n","        validation_split=0.2,\n","    )\n","\n","    train_generator = train_datagen.flow_from_directory(\n","        dataset_path,\n","        target_size=(img_width, img_height),\n","        batch_size=batch_size,\n","        class_mode='categorical',\n","        subset='training'\n","    )\n","\n","    validation_generator = train_datagen.flow_from_directory(\n","        dataset_path,\n","        target_size=(img_width, img_height),\n","        batch_size=batch_size,\n","        class_mode='categorical',\n","        subset='validation'\n","    )\n","\n","    return train_generator, validation_generator\n","\n","# 하이퍼파라미터 및 튜너 설정\n","hyperparameters = keras_tuner.HyperParameters()\n","\n","# 데이터 생성기 생성\n","train_generator, validation_generator = create_generators(hyperparameters)\n","\n","\n","# 튜너 설정\n","tuner = RandomSearch(\n","    build_model,\n","    objective='val_accuracy',\n","    max_trials=15,  # 시도할 최대 트라이얼 수\n","    executions_per_trial=3,  # 각 트라이얼마다의 실행 횟수\n","    # 튜닝 결과 저장 경로\n","    directory='/content/drive/MyDrive/00_05_4_daejeon_3/2023.12.26 프로젝트/HYUN/hpt',\n","    # 폴더 이름\n","    project_name='ResNet50_hpt_2',\n","    hyperparameters=hyperparameters,\n","    overwrite=False  # 이전 튜닝 세션을 재개하기 위해 False로 설정\n",")\n","\n","\n","# 튜너 객체 상태 불러오기\n","tuner.reload()\n","\n","# 최적의 하이퍼파라미터 불러오기\n","best_hps = tuner.get_best_hyperparameters(num_trials=2)[1]\n","\n","# 모델 구성 및 추가 학습\n","model = build_model(best_hps)\n","\n","\n","# 에포크 수와 파일 저장 경로 설정\n","epochs = 100  # 원하는 에포크 수로 설정 가능\n","\n","\n","model_save_path = '/content/drive/MyDrive/00_05_4_daejeon_3/2023.12.26 프로젝트/HYUN/model/best_restnet4'  # 모델 저장 경로 설정\n","\n","\n","# 조기 종료 콜백 설정\n","early_stopping_callback = EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n","# 체크포인트 콜백 설정\n","modelname = \"ResNet50_{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n","checkpoint_path = os.path.join(model_save_path, modelname)\n","checkpoint_callback = ModelCheckpoint(\n","    checkpoint_path,\n","    monitor='val_accuracy',\n","    verbose=1,\n","    save_best_only=True,\n","    mode='max'\n",")\n","\n","# 모델 학습\n","model.fit(\n","    train_generator,\n","    epochs=epochs,\n","    validation_data=validation_generator,\n","    callbacks=[early_stopping_callback, checkpoint_callback]\n",")\n","\n","# 모델 평가\n","validation_loss, validation_accuracy = model.evaluate(validation_generator)\n","print(f\"Validation Loss: {validation_loss}\")\n","print(f\"Validation Accuracy: {validation_accuracy}\")"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":476,"status":"ok","timestamp":1703572774884,"user":{"displayName":"JW P","userId":"05164750983755444081"},"user_tz":-540},"id":"0C2T7v882Po9","outputId":"049be06b-448e-4ef3-e1a3-e21137da6bf1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Best Model: ResNet50_09-0.8859.hdf5\n","Accuracy: 0.8859\n"]}],"source":["import os\n","import re\n","\n","# 모델 저장 경로\n","model_save_path = '/content/drive/MyDrive/00_05_4_daejeon_3/2023.12.26 프로젝트/HYUN/model/best_restnet4'\n","\n","# 체크포인트 파일들의 리스트\n","checkpoint_files = os.listdir(model_save_path)\n","\n","# 정확도를 저장할 딕셔너리\n","accuracy_dict = {}\n","\n","# 각 파일명에서 정확도 추출\n","for file in checkpoint_files:\n","    # 파일명에서 정확도 값을 추출하는 정규 표현식\n","    match = re.search(r\"(\\d+\\.\\d+)\\.hdf5$\", file)\n","    if match:\n","        accuracy = float(match.group(1))\n","        accuracy_dict[file] = accuracy\n","\n","# 가장 높은 정확도를 가진 모델 찾기\n","best_model_file = max(accuracy_dict, key=accuracy_dict.get)\n","best_accuracy = accuracy_dict[best_model_file]\n","\n","print(f\"Best Model: {best_model_file}\")\n","print(f\"Accuracy: {best_accuracy}\")\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":14903,"status":"ok","timestamp":1703572838078,"user":{"displayName":"JW P","userId":"05164750983755444081"},"user_tz":-540},"id":"x4n-1kxc02De"},"outputs":[],"source":["import os\n","from tensorflow.keras.models import load_model\n","\n","\n","# 모델 불러오기\n","model = load_model(os.path.join(model_save_path, best_model_file))"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":594,"status":"ok","timestamp":1703572882470,"user":{"displayName":"JW P","userId":"05164750983755444081"},"user_tz":-540},"id":"IGnO1EX7AFRW","outputId":"82c74734-56df-48d1-ded0-8265bcc694a8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 5755 images belonging to 5 classes.\n"]}],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# 데이터셋 경로\n","dataset_path = '/content/drive/MyDrive/00_05_4_daejeon_3/2023.12.26 프로젝트/data/current_dataset'\n","\n","# 이미지 크기 및 배치 크기 설정\n","img_width, img_height = 224, 224  # ResNet50의 기본 이미지 크기\n","batch_size = 64\n","\n","# 이미지 데이터 생성기\n","train_datagen = ImageDataGenerator(\n","    rescale=1./255,\n","    rotation_range=10,\n","    width_shift_range=0.1,\n","    height_shift_range=0.1,\n","    shear_range=0.1,\n","    zoom_range=0.1,\n","    horizontal_flip=True,\n","    fill_mode='nearest',\n","    validation_split=0.2\n",")\n","\n","# 훈련 및 검증 데이터 생성기\n","train_generator = train_datagen.flow_from_directory(\n","    dataset_path,\n","    target_size=(img_width, img_height),\n","    batch_size=batch_size,\n","    class_mode='categorical',\n","    subset='training'\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1d0AIUDPkzztBxHa7DKdyEAAr_J_sJ1zn"},"id":"jdWiBBlM23SO","outputId":"ba43c7df-d27b-4be6-bae3-5b3b3c86e599"},"outputs":[],"source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import tensorflow as tf\n","\n","# TensorFlow 로깅 레벨 설정 (중간 메시지 숨김)\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","tf.get_logger().setLevel('ERROR')\n","\n","# 테스트 데이터셋 경로와 모델 경로 설정\n","test_dataset_path = '/content/drive/MyDrive/00_05_4_daejeon_3/2023.12.26 프로젝트/data/test_dataset'\n","model = load_model(os.path.join(model_save_path, best_model_file))\n","\n","\n","# 테스트 데이터셋에 대한 ImageDataGenerator 생성\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","test_generator = test_datagen.flow_from_directory(\n","    test_dataset_path,\n","    target_size=(224, 224),\n","    batch_size=1,\n","    class_mode='categorical',\n","    shuffle=False\n",")\n","\n","# 클래스 인덱스 가져오기\n","class_indices = test_generator.class_indices\n","class_names = list(class_indices.keys())\n","\n","# 모든 예측을 수행하고 결과를 저장\n","correct_predictions = {}\n","incorrect_predictions = {}\n","for _ in range(len(test_generator)):\n","    x, y = next(test_generator)\n","    image = x[0]  # 이미지 데이터\n","    true_label = class_names[np.argmax(y[0])]\n","    prediction = model.predict(x, verbose=0)\n","    predicted_class = class_names[np.argmax(prediction)]\n","\n","    # 여기서 image 대신 x[0]을 저장합니다.\n","    if true_label == predicted_class:\n","        if true_label not in correct_predictions:\n","            correct_predictions[true_label] = []\n","        correct_predictions[true_label].append((image, predicted_class))  # 이미지 데이터 저장\n","    else:\n","        if true_label not in incorrect_predictions:\n","            incorrect_predictions[true_label] = []\n","        incorrect_predictions[true_label].append((image, predicted_class))  # 이미지 데이터 저장\n","\n","# 시각화 함수 정의\n","def visualize_predictions(predictions, title):\n","    num_images = sum(len(v) for v in predictions.values())\n","    plt.figure(figsize=(10, num_images // 4 * 4))  # 가로 10인치, 세로는 이미지 수에 비례하게 조정\n","\n","    i = 1\n","    for class_name, images in predictions.items():\n","        for img, pred_class in images:\n","            plt.subplot(num_images // 4 + 1, 4, i)\n","            plt.imshow(img)\n","            if class_name != pred_class:\n","                plt.title(f\"{class_name} as {pred_class}\", size=14, color='red')\n","                plt.gca().add_patch(plt.Rectangle((0, 0), 223, 223, fill=False, edgecolor='red', lw=2))\n","            else:\n","                plt.title(f\"Correct: {class_name}\", size=14)\n","            plt.axis('off')\n","            i += 1\n","    plt.suptitle(title, size=16)\n","    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # 상단 제목에 여백을 줍니다.\n","    plt.show()\n","\n","# 맞춘 예측 시각화\n","visualize_predictions(correct_predictions, \"correct_predictions\")\n","\n","# 틀린 예측 시각화\n","visualize_predictions(incorrect_predictions, \"incorrect_predictions\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m6pjErN926uk"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","\n","# 전체 정확도를 위한 리스트\n","y_true = []\n","y_pred = []\n","\n","# 각 카테고리별 정확도를 위한 딕셔너리\n","category_true = {category: [] for category in class_names}\n","category_pred = {category: [] for category in class_names}\n","\n","# 이미지 데이터를 배치 단위로 불러오기\n","for _ in range(len(test_generator)):\n","    x, y = next(test_generator)\n","    true_category = class_names[np.argmax(y[0])]\n","    y_true.append(true_category)\n","\n","    # 모델 예측\n","    prediction = model.predict(x, verbose=0)\n","    predicted_category = class_names[np.argmax(prediction)]\n","    y_pred.append(predicted_category)\n","\n","    # 카테고리별 리스트에 추가\n","    category_true[true_category].append(true_category)\n","    category_pred[true_category].append(predicted_category)\n","\n","# 전체 정확도 계산\n","total_accuracy = accuracy_score(y_true, y_pred)\n","\n","# 각 카테고리별 정확도 계산\n","category_accuracy = {}\n","for category in class_names:\n","    category_accuracy[category] = accuracy_score(category_true[category], category_pred[category])\n","\n","# 결과 출력\n","print(f\"Total Accuracy: {total_accuracy}\")\n","for category, acc in category_accuracy.items():\n","    print(f\"Accuracy for {category}: {acc}\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNdTwgq03IX1M2FxehhdpUi","machine_shape":"hm","mount_file_id":"1OBnJWnWSv9gAWo_JPo3AuD2yLEVOPFnl","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}